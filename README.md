<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#about-the-repo">About the Repo</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#known-issues-and-workarounds">Known Issues and Workarounds</a></li>
  </ol>
</details>


<!-- ABOUT THE PROJECT -->
## About the Repo

This branch of the repo is used to execute a pick-and-place task for a grasp pose generated by a grasp detection algoritm. The task is constructed using moveit-task-constructor based on the example from the [moveit2_tutorials](https://github.com/moveit/moveit2_tutorials/blob/humble/doc/tutorials/pick_and_place_with_moveit_task_constructor/src/mtc_node.cpp).

### Installation

1. Clone the repo
   ```sh
   git clone --recurse-submodules -j8 https://git-ce.rwth-aachen.de/wzl-mq-ms/docker-ros/ros2/kinova-ros2.git --branch devel_kedar
   ```
3. Build the Docker Image
   ```sh
   bash docker_build.sh
   ```

### Usage

1. Start the kortex vision module
   ```sh
   bash docker_run/run_kinova_vision.sh
   ```
   This will launch a node to start the camera drivers.

2. Start the pick-and-place task
   ```sh
   bash docker_run/run_kinova_grasp.sh
   ```
   This will start a node which will execute the pick-and-place task as soon as a grasp pose is published to a topic.

3. Start the image pre-processing and grasp post-processing module
   ```sh
   bash docker_run/run_kinova_ops.sh 
   ```
   This will start a node which will save the RGBD images to a shared location and read the grasp poses file to select the most confident grasp pose.

4. When physical robot is not available start a sample grasp pose publisher, instead of `kinova_vision` and `kinova_ops`.
   ```sh
   bash docker_run/run_py_pubsub_talker.sh
   ```
   This will start a node which will publish a fixed grasp pose at a fixed interval to a topic.
   
### Contact GraspNet

1. Clone the Contact GraspNet repo in the parent directory
   ```sh
   git clone https://git-ce.rwth-aachen.de/wzl-mq-ms/docker-ros/deep-learning/grasping-for-gg-cnn/contact-graspnet.git
   ```

2. The file structure should look like the following. Create an additional empty folder to facilitate file transfer.
    ```
    kinova
    ├── kinova-ros2
    ├── contact-graspnet
    ├── kinova-transfer
    ```

3. Build the Docker Image
   ```sh
   bash docker_build.sh
   ```

4. Start the process that waits for the images to be saved in kinova-transfer folder then generates and saves grasps back in the same folder
   ```sh
   bash watch_and_process.sh
   ```

### Known Issues and Workarounds

If the moveit_example fails to execute then check the error messages on both the terminal windows one for the driver/gazebo and the other for the moveit. Those could be similar to one of the following:

1. Deviation between actual robot start pose and start pose from trajectory being executed
    - move the end effector a few centimeters using xbox controller and execute the script again
2. Velocity/Acceleration values out of bound
    - modify the scaling factors for velocity and acceleration in the moveit config files
3. Load too heavy / max power
    - try reducing the load first or move it closer to base (need to generate trajectories again for new pickup drop poses)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

