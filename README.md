<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#about-the-repo">About the Repo</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#known-issues-and-workarounds">Known Issues and Workarounds</a></li>
  </ol>
</details>


<!-- ABOUT THE PROJECT -->
## About the Repo

This branch of the repo is used to execute a pick-and-place task for a grasp pose generated by a grasp detection algoritm. The task is constructed using moveit-task-constructor based on the example from the [moveit2_tutorials](https://github.com/moveit/moveit2_tutorials/blob/humble/doc/tutorials/pick_and_place_with_moveit_task_constructor/src/mtc_node.cpp).

### Installation

1. Clone the repo
   ```sh
   git clone --recurse-submodules -j8 https://git-ce.rwth-aachen.de/wzl-mq-ms/docker-ros/ros2/kinova-ros2.git --branch devel_kedar
   ```
3. Build the Docker Image
   ```sh
   bash docker_build.sh
   ```

### Usage

1. Start the kortex vision module
   ```sh
   bash docker_run/run_kinova_vision.sh
   ```
   This will launch a node to start the camera drivers.

2. Start the pick-and-place task
   ```sh
   bash docker_run/run_kinova_grasp.sh
   ```
   This will start a node which will execute the pick-and-place task as soon as a grasp pose is published to a topic.

3. Start the image pre-processing and grasp post-processing module
   ```sh
   bash docker_run/run_kinova_ops.sh 
   ```
   This will start a node which will save the RGBD images to a shared location and read the grasp poses file to select the most confident grasp pose.

4. When physical robot is not available start a sample grasp pose publisher, instead of `kinova_vision` and `kinova_ops`.
   ```sh
   bash docker_run/run_py_pubsub_talker.sh
   ```
   This will start a node which will publish a fixed grasp pose at a fixed interval to a topic.
   
### Contact GraspNet

1. Clone the Contact GraspNet repo in the parent directory
   ```sh
   git clone https://git-ce.rwth-aachen.de/wzl-mq-ms/docker-ros/deep-learning/grasping-for-gg-cnn/contact-graspnet.git --branch devel_kedar
   ```

2. Build the Docker Image
   ```sh
   bash docker_build.sh
   ```

3. The file structure should look like the following. Make an additional empty directory to facilitate file transfer.
    ```
    kinova
    ├── kinova-ros2
    ├── contact-graspnet
    ├── kinova-transfer
    ```

4. Start the Docker container
   ```sh
   bash docker_run/docker_run.sh
   ```

5. Start the process that waits for the images to be saved in kinova-transfer folder then generates and saves grasps back in the same folder
   ```sh
   bash watch_and_process.sh
   ```

### Known Issues and Workarounds

1. The generated trajectories for the pick-and-place consitently put joint_6 to its limit, this causes low level safety check on the robot to 
   stop the execution of the plan.
2. The generated trajectories are not the shortest trajectories, they often follow a very long path.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

